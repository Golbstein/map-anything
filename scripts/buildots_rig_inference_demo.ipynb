{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Buildots Rig: Minimal inference demo\n",
        "\n",
        "This notebook loads Buildots data via **BuildotsRigAdapter**, then runs:\n",
        "1. **Original MapAnything** (pretrained, outputs full 6-DoF poses)\n",
        "2. **Rig MapAnything** (rig config, outputs xyz + yaw)\n",
        "\n",
        "**Setup:** Set `BUILDOTS_PYCODE_PATH` and `DATA_ROOT` below to your Buildots repo and dataset path."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Path to your Buildots pycode (for BuildotsDataset)\n",
        "BUILDOTS_PYCODE_PATH = \"/Users/jenia/projects/buildots/pycode\"  # adjust as needed\n",
        "# Buildots dataset root (segment folders)\n",
        "DATA_ROOT = \"/bd-resources/jenia/dataset/buildots_da3\"  # adjust as needed\n",
        "\n",
        "import sys\n",
        "if BUILDOTS_PYCODE_PATH and BUILDOTS_PYCODE_PATH not in sys.path:\n",
        "    sys.path.insert(0, BUILDOTS_PYCODE_PATH)\n",
        "\n",
        "import torch\n",
        "from mapanything.datasets.buildots_rig import BuildotsRigAdapter, buildots_rig_collate_fn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Buildots data with BuildotsRigAdapter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from research.positioning_net.buildots_dataset_generator import BuildotsDataset\n",
        "\n",
        "buildots_ds = BuildotsDataset(\n",
        "    root_dir=DATA_ROOT,\n",
        "    seq_len=10,\n",
        "    image_size=(350, 350),\n",
        "    fov=90,\n",
        "    debug=False,\n",
        "    training=False,\n",
        ")\n",
        "\n",
        "adapter = BuildotsRigAdapter(\n",
        "    buildots_ds,\n",
        "    num_timestamps=3,\n",
        "    data_norm_type=\"dinov2\",  # MapAnything encoder expects dinov2\n",
        ")\n",
        "\n",
        "print(f\"Adapter length: {len(adapter)}\")\n",
        "print(f\"Views per sample: {3 * 4}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def add_batch_dim(views):\n",
        "    \"\"\"Turn a list of per-view dicts (single sample) into batch-of-1 format for model.forward.\"\"\"\n",
        "    batch = []\n",
        "    for v in views:\n",
        "        out = {}\n",
        "        for k, val in v.items():\n",
        "            if isinstance(val, torch.Tensor):\n",
        "                out[k] = val.unsqueeze(0)\n",
        "            elif isinstance(val, str):\n",
        "                out[k] = [val]\n",
        "            else:\n",
        "                out[k] = val\n",
        "        batch.append(out)\n",
        "    return batch\n",
        "\n",
        "# Get one sample: 12 views (3 timestamps x 4 cameras)\n",
        "views_single = adapter[0]\n",
        "batch_views = add_batch_dim(views_single)\n",
        "print(f\"Batch: {len(batch_views)} views, img shape: {batch_views[0]['img'].shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Original MapAnything (pretrained)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from mapanything.models import MapAnything\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_orig = MapAnything.from_pretrained(\"facebook/map-anything\").to(device).eval()\n",
        "\n",
        "# Move batch to device\n",
        "batch_views_device = []\n",
        "for v in batch_views:\n",
        "    batch_views_device.append(\n",
        "        {k: v[k].to(device) if isinstance(v[k], torch.Tensor) else v[k] for k in v}\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with torch.no_grad():\n",
        "    preds_orig = model_orig(batch_views_device)\n",
        "\n",
        "print(\"Original model outputs (per-view):\")\n",
        "p0 = preds_orig[0]\n",
        "print(f\"  cam_trans: {p0.get('cam_trans', 'N/A')}\")\n",
        "print(f\"  cam_quats: {p0.get('cam_quats', 'N/A')}\")\n",
        "if \"cam_trans\" in p0:\n",
        "    print(f\"  cam_trans shape: {p0['cam_trans'].shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Rig MapAnything (xyz + yaw)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import hydra\n",
        "from omegaconf import OmegaConf\n",
        "from mapanything.models import model_factory\n",
        "\n",
        "# Build full rig config: base MapAnything (encoder, transformer) + rig head & geometric inputs\n",
        "pkg_dir = os.path.dirname(os.path.abspath(__import__(\"mapanything\").__file__))\n",
        "project_root = os.path.dirname(pkg_dir)\n",
        "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
        "with hydra.initialize(version_base=None, config_path=os.path.join(project_root, \"configs\")):\n",
        "    cfg = hydra.compose(config_name=\"train\", overrides=[\"model=mapanything\"])\n",
        "base_model_config = OmegaConf.to_container(cfg.model.model_config, resolve=True)\n",
        "rig_config_path = os.path.join(pkg_dir, \"configs\", \"model\", \"mapanything_rig.yaml\")\n",
        "rig_cfg = OmegaConf.load(rig_config_path)\n",
        "rig_overrides = OmegaConf.to_container(rig_cfg.model.model_config, resolve=True)\n",
        "merged_config = {**base_model_config, **rig_overrides}\n",
        "\n",
        "model_rig = model_factory(\"mapanything\", **merged_config).to(device).eval()\n",
        "\n",
        "# Optional: load a trained rig checkpoint\n",
        "# ckpt = torch.load(\"path/to/checkpoint-best.pth\", map_location=\"cpu\", weights_only=False)\n",
        "# model_rig.load_state_dict(ckpt[\"model\"], strict=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with torch.no_grad():\n",
        "    preds_rig = model_rig(batch_views_device)\n",
        "\n",
        "print(\"Rig model outputs (per-view):\")\n",
        "p0_rig = preds_rig[0]\n",
        "print(f\"  cam_trans: {p0_rig.get('cam_trans', 'N/A')}\")\n",
        "print(f\"  cam_yaw (cos, sin): {p0_rig.get('cam_yaw', 'N/A')}\")\n",
        "if \"cam_trans\" in p0_rig:\n",
        "    print(f\"  cam_trans shape: {p0_rig['cam_trans'].shape}\")\n",
        "if \"cam_yaw\" in p0_rig:\n",
        "    print(f\"  cam_yaw shape: {p0_rig['cam_yaw'].shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Compare (first 4 views = anchor at t=0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"View 0 (anchor):\")\n",
        "print(f\"  Original cam_trans: {preds_orig[0]['cam_trans'].cpu().squeeze()}\")\n",
        "print(f\"  Rig      cam_trans: {preds_rig[0]['cam_trans'].cpu().squeeze()}\")\n",
        "print(f\"  Rig      cam_yaw:   {preds_rig[0]['cam_yaw'].cpu().squeeze()}\")\n",
        "print(\"GT (anchor):\")\n",
        "print(f\"  camera_pose_trans_gt: {batch_views[0]['camera_pose_trans_gt']}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}